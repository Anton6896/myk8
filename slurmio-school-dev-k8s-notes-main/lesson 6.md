---
date created: 2021-10-19 19:12:20 (+03:00), Tuesday
---

# Урок 6: Сетевые абстракции. Probes
- [Запись](https://www.youtube.com/watch?v=OmTYdf_uDeQ) на youtube
- [Презентация](https://github.com/Slurmio/school-dev-k8s/blob/main/lecture/6.Network_abstractions.pdf) лекции в [github репозитории](https://github.com/Slurmio/school-dev-k8s) школы

## Освежаем знания по базовым абстракциям [00:00:16](https://youtu.be/OmTYdf_uDeQ?t=16)

### Pod
- В kubernetes минимальная абстракция - это не контейнер, а pod
- Pod может состоять минимум из 2х контейнеров
    - один контейнер - это непосредственно рабочая нагрузка, внутри запущено наше приложение
    - второй контейнер - так называемый служебный контейнер, он держит в себе сетевой неймспейс, [процесс pause](https://stackoverflow.com/questions/48651269/what-are-the-pause-containers), его мы особо не касаемся на практике
- Более того, сама абстракция pod тоже служебная, запуская свое приложение с помощью пода мы ставим себя в очень неудобное положение, т.к. лишаем себя гибкости а также двух важных вещей в контексте управления приложением:
    - первое - это обновление приложения, допустим, у нас вышла новая версия приложения, нам нужно каким-то образом сделать рестарт пода, чтобы он пересоздался, и на уровне пода это неудобно
    - второе - это уменьшение/увеличение количества реплик то есть скейлинг - все приходится делать вручную, это неудобно
- Таким образом, pod это служебная абстракция, на её уровень приходится спускаться, когда дело касается какого-то дебага, например, посмотреть логи

### ReplicaSet [00:03:16](https://youtu.be/OmTYdf_uDeQ?t=196)
- Эта абстракция с помощью которой достаточно удобно можно приложение скейлить
- На уровне репликасетов мы можем удобно, централизовано, с помощью одной команды увеличивать и уменьшать количество реплик одного приложения, это здорово, это хорошо и решает одну из двух таких ключевых задач по управлению нашим приложением - скейлинг
- Но с помощью репликасетов неудобно обновлять наше приложение, нам приходится все равно руками как-то там лезть, в общем неудобно, поэтому репликасетами мы тоже не пользуемся, это тоже, скорее, служебная абстракция, ею пользуемся наверное еще реже даже чем подом, потому что это прям совсем такая служебная-служебная, отвечает она только за управление скейлингом

### Deployment [00:04:17](https://youtu.be/OmTYdf_uDeQ?t=257)
- Это действительно та абстракция с помощью которой можно запускать приложение, это действительно так, потому что deployment решает две вот эти самые задачи управления приложением - управления репликами и обновления вашего приложения, все можно делать с помощью деплоймента, всё описывается в yaml манифесте, всё управляется с помощью kubectl, всё достаточно удобно

## Про вступление [00:04:45](https://youtu.be/OmTYdf_uDeQ?t=285)
- Вот это небольшое вступление к теме было нужно, потому что помимо этих двух задач, обновление приложения и скейлинг, есть еще ряд задач, которые не менее важны
- Одна из этих задач - это понимать, а действительно ли приложение которое сейчас запущено в кластере, оно работает и с ним все хорошо
    - Это очень хороший вопрос, очень важный вопрос, нам нужно понимать что действительно, трафик, который идет на наше приложение, он там обрабатывается и с ним всё хорошо
- Может быть такое чисто технически, что приложение висит, процесс как бы есть, он с PID 1 где-то в контейнере крутится, но по факту функциональность не осуществляется, то есть приложение не работает, это может быть случай, например, какого нибудь дедлока, и чтобы такие моменты контролировать kubernetes нам предлагает такое решение как пробы, это возможность как раз понимать, что происходит с приложением, действительно ли оно работает, действительно ли оно готово принимать трафик и действительно ли оно запустилось
- Для такого контроля у нас есть три вида этих проб - это лайвнес проба, рединес проба и стартап проба

## Probes [00:07:09](https://youtu.be/OmTYdf_uDeQ?t=429)
- Синоним хелсчеков
- Позволяют проверять, что приложение действительно работает

### Liveness Probe [00:07:19](https://youtu.be/OmTYdf_uDeQ?t=439)
- Контроль за состоянием приложения во время его жизни
- Исполняется постоянно, с заданной периодичностью
- Если такая проверка будет провалена, то приложение (под) будет перезапущено

### Readiness Probe [00:08:06](https://youtu.be/OmTYdf_uDeQ?t=486)
- Проверяет, готово ли приложение применять трафик
- В случае неудачного выполнения приложение убирается из балансировки, соответственно, после этого в данный инстанс приложения перестанет идти трафик
- Исполняется постоянно, с заданной периодичностью

### Startup Probe [00:09:10](https://youtu.be/OmTYdf_uDeQ?t=550)
- Проверяет, запустилось ли приложение вообще
- Исполняется при старте, остальные типы проверок начнут выполнятся после завершения проверки данного типа

### Переходим в консоль [00:14:07](https://youtu.be/OmTYdf_uDeQ?t=847)
- Работаем в каталоге `~/school-dev-k8s/practice/7.network-abstractions/1.probes`
- Смотрим манифест деплоймента:
```yaml
---
# file: practice/1.kube-basics-lecture/4.resources-and-probes/deployment-with-stuff.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: quay.io/testing-farm/nginx:1.12
        name: nginx
        ports:
        - containerPort: 80
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 80
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 80
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
          initialDelaySeconds: 10
        startupProbe:
          httpGet:
            path: /
            port: 80
          failureThreshold: 30
          periodSeconds: 10
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
...
```
- Probes описываются в спецификации внутри контейнеров
- Для каждого контейнера своё описание probes
- Рассматриваем настройки probes
    - readinessProbe
        - failureThreshold
            - допустимое количество проваленных попыток подряд, прежде чем приложение будет выведено из балансировки
            - применяется для того, чтобы из-за каких-то небольших проблем, например с сетью, приложение не останавливалось полностью
        - httpGet
            - сама проверка, в данном случае мы идём в корневой локейшн нашего приложения по 80 порту, для проверки, что nginx готов принимать трафик, этого достаточно
            - успешными считаются коды ответа в диапазоне от 200 до 399
                - например, 301 - это ОК, а 400, 404 - уже не ОК
            - помимо httpGet есть еще проверки:
                - exec
                    - с помощью неё мы можем выполнить внутри контейнера какую-то команду, например, если это база данных, мы можем выполнить SELECT 1, таким образом убедиться, что БД поднялась и готова принимать запросы
                - tcpSocket
                    - это самая простая проверка, которая позволяет проверять TCP сокет, стучаться в него, если он открыт и работает - значит проверка прошла, всё ОК
        - periodSeconds
            - означает, с какой периодичностью выполнять проверку
        - successThreshold
            - сколько успешных проверок сбросит счётчик failureThreshold
        - timeoutSeconds
            - ограничение на время выполнения проверки, собственно, таймаут
    - livenessProbe
        - большинство настроек аналогичны описанным выше
        - initialDelaySeconds
            - отсрочка выполнения первой проверки, использовали до появления startupProbe
    - startupProbe
        - основное отличие от предыдущих проверок - большие значения, т.е. мы даём сервису 5 минут на запуск
- При выводе k get pod, в колонке READY выводится результат readinessProbe
- Если сходить в логи пода, развёрнутого из этого манифеста деплоймента, можно увидеть информацию о запросах probes к нашему nginx

## Способы публикации [00:28:32](https://youtu.be/OmTYdf_uDeQ?t=1712)
- Как опубликовать приложение, запущенное в кластере kubernetes, чтобы клиенты, находящиеся вне кластера, могли получить к нему доступ?
- Как настроить внутрикластерное взаимодействие своих приложений, фронтенда, бэкенда?
- Унас есть 2 абстракции, с помощью которых можно решать данные задачи, о них ниже

### Service [00:29:23](https://youtu.be/OmTYdf_uDeQ?t=1763)
- Ключевые моменты почти для всех сервисов
    - Важно, чтобы селектор совпадал с лейблами подов
    - Поды и сервисы должны находиться в одном неймспейсе
- ClusterIP [00:29:42](https://youtu.be/OmTYdf_uDeQ?t=1782)
    - Используется для того, чтобы наладить внутрикластерное взаимодействие частей приложения (например, связать фронтенд с бэкендом)
    - В теории, можно отказаться от использования данного сервиса и работать с IP адресами подов, передавать их, например, через переменые окружения, но это будет работать до тех пор, пока поды не начнут перезапускаться и, соответственно, менять адреса
    - Также, с помощью данного сервиса мы можем пробрасывать приложение через kubectl port-forward на хост, с которго запускается данная команда, это удобно для разработки или отладки. Таким образом удобно работать с зависимостями, можно обращаться к тем компонентам кластера, которые требуются в данный момент
        - Команда `kubectl port-forward service/my-service 10000:80` пробросит 80 порт сервиса my-service на 10000 порт хоста, с которого запускается эта команда
        - Раньше подобное можно было делать командой kubectl proxy, но сейчас этот метод считается устаревшим
- NodePort [00:34:22](https://youtu.be/OmTYdf_uDeQ?t=2062)
    - [Документация](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport)
    - Позволяет опубликовать приложение наружу, для этого он и используется, правда, с оговорками
    - Может публиковать приложение только на определённом диапазоне портов, по умолчанию это 30000-32767
    - При необходимости можно явно указать нужный порт в манифесте
    - Создаёт на каждой ноде кластера соответствующее правило трансляции
    - Подходит для использования, если есть внешний балансировщик, в котором можно указать соответствие
    - Может использоваться для работы с LoadBalancer [00:38:09](https://youtu.be/OmTYdf_uDeQ?t=2289)
    - [00:46:43](https://youtu.be/OmTYdf_uDeQ?t=2803) Реальный кейс применения NodePort - переносили монолитные приложения, их пилили на микро сервисы, затаскивали всё в kubernetes, получалось что часть приложения в монолите, а часть приложения в kubernetes, при этом есть некий центральный nginx, который часть трафика продолжает отправлять на монолит, а часть трафика отправляет уже в kubernetes, на порты, опубликованные наружу через NodePort. Например, \<url>/api отправляли в kubernetes а \<url>/cabinet отправляли еще в монолит и вот таким образом можно решать подобные проблемы
- LoadBalancer [00:38:20](https://youtu.be/OmTYdf_uDeQ?t=2300)
    - Тип LoadBalancer используется, преимущественно, у облачных провайдеров и как раз этот тип сервиса нам позволяет действительно легко и прозрачно предоставлять доступ извне кластера к нашему приложению
    - Данный тип сервиса используется преимущественно в облаках, потому что у облачных провайдеров есть контроллер, например openstack контроллер, который постоянно смотрит в kubernetes и контролирует создание сервисов типа LoadBalancer и когда данный тип сервиса создается в кластере, контроллер у себя создает балансировщик, например elastic LoadBalancer, и трафик отправляется в этот балансировщик и уже соответственно с этого балансировщика отправляются внутрь кластера на наше приложение
    - И вот этот балансировщик, он создается контроллером и если контроллера нет, соответственно, нет функциональности, которая позволяет смотреть в API кубернетеса и контролировать создание сервисов типа LoadBalancer, то никакой магии не произойдет и сервис типа LoadBalancer не будет работать, то есть, на bare metal по умолчанию это не работает, он просто будет висеть
    - Но в VK Cloud это будет работать, поэтому всё хорошо, единственное у нас всё-таки кластер немного кастомизирован и всё такое, поэтому могут всплыть некоторые особенности
    - Вообще, в облаках это работает, это очень удобно, более того, есть возможность в манифесте указать  конкретный статический IP адрес, который будет использоваться как входная точка в ваш балансировщик
    - LoadBalancer отлично подходит чтобы опубликовать свое приложение и в том числе TCP, например, если надо выставить наружу кластера какой-нибудь postgres или rabbitmq, то LoadBalancer отличный вариант, но с учётом того, что работает это в облаках
- ExternalName [00:40:58](https://youtu.be/OmTYdf_uDeQ?t=2458)
    - ![externalName](./assets/lesson_6/externalName.png)
    - Позволяет создать алиас для DNS имени в качестве имени сервиса
    - Посмотрите внимательно на стрелки, если раньше у нас стрелки шли от сервиса к поду, то теперь это происходит немножко наоборот и это не ошибка это специфика работы этого сервиса
    - Когда мы создаем вот такой сервис, как видите, он достаточно минималистичный, создается правило и создается dns-запись и, в таком случае, когда мы из пода обращаемся по имени сервиса, например через `curl my-service` или `nslookup my-service` то мы по факту попадаем на example.com, адрес который мы прописали в поле ExternalName, то есть, это некая такая хитрая переадресация
    - Сложно придумать кейсы где это можно действительно применить, потому что обычно ничто не мешает сразу же из пода напрямую обращаться к example.com, но это может быть, к примеру, какой то закрытый продукт где вы ничего не можете править и вам нужно там что-то поменять или какие-то значения в приложении захардкожены и вам нужно их сменить
- ExternalIPs [00:42:39](https://youtu.be/OmTYdf_uDeQ?t=2559), технические шоколадки и повтор с [00:44:58](https://youtu.be/OmTYdf_uDeQ?t=2698)
    - ![externalIPs](./assets/lesson_6/externalIPs.png)
    - Если вы посмотрите на схему то увидите, что тут почти то же самое что и у сервисов NodePort, только вместо портов здесь указаны IP
    - Схема того как работает сервис типа ExternalIPs очень похожа на то, как работает сервис типа NodePort, c тем исключением, что в NodePort у нас создается правило трансляции которые работают с каким-то конкретным портом, а в ExternalIPs у нас создается правило которое работает с IP адресом
    - Когда мы создаем сервис ExternalIP, у нас на каждой ноде кластера создаются правила трансляции и трафик, приходящий на IP адрес, указанный в этом поле, будет пробрасываться в наше приложение.
    - Кластер kubernetes такой умный и понимает где именно этот IP адрес находится? Не понимает и это не очень удобно
    - Один из примеров применения данного сервиса - у нас есть какой-то внешний балансировщик и есть какой-то виртуальный IP-адрес (VIP) или, как принято говорить, протокол VRRP, keepalived настроен, допустим наш VIP изначально находится на первой ноде, вдруг с ней что-то произошло и тогда keepalived по VRRP этот адрес переносит на вторую ноду и мы можем быть уверены что на этой ноде все необходимые правила трансляции тоже есть, то есть, трафик будет приходить и сразу попадать в нужный под
- Headless [00:47:58](https://youtu.be/OmTYdf_uDeQ?t=2878), техническая заминка и повтор с [00:50:56](https://youtu.be/OmTYdf_uDeQ?t=3056)
    - ![headless](./assets/lesson_6/headless.png)
    - Этот сервис не про внутрикластерное взаимодействие и не про публикацию, поэтому он немножко так особняком стоит
    - Отличительной особенностью Headless сервиса является то что у него не указан ClusterIP (см. слайд)
    - Для такого сервиса не будет создан IP адрес, но будет создана DNS запись, которая будет отдавать IP адреса всех подов для этого сервиса
    - Используя данную DNS запись мы также можем обращаться напрямую к подам
        - Будет выглядеть примерно так: my-app-1.my-headless-service.my-cluster.my-zone
    - Данный сервис используется в конкретном случае, а именно со StatefulSet
        - [00:53:30](https://youtu.be/OmTYdf_uDeQ?t=3210) StatefulSet является заменой деплойменту для stateful приложений, например баз данных, эта тема будет рассмотрена в следующих лекциях
        - У подов в StatefulSet имена подов формируются добавлением номера в конце, таким образом, можно настроить репликацию, чтобы отправлять запросы на чтение в slave, а запросы на запись в master, определяя, какой запрос куда отправлять, опираясь на имена подов и DNS запись, сформированную headless сервисом

### Практика [00:54:13](https://youtu.be/OmTYdf_uDeQ?t=3253)
- Работаем в каталоге `~/school-dev-k8s/practice/6.network-abstractions/2.ingress-and-services`

#### Разбираем сервис ClusterIP
- Смотрим на манифест сервиса ClusterIP
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: my-service  # имя сервиса
    spec:
      ports:
      - port: 80        # порт, на котором сервис будет принимать трафик
        targetPort: 80  # порт, на который сервис будет перенаправлять трафик
      selector:
        app: my-app     # значение метки, по которой селектор будет производить выборку
      type: ClusterIP   # тип сервиса
    ```
    - В данном случае тип сервиса указан, но если тип не указывать, то по умолчанию будет использоваться ClusterIP
    - Важно корректно указывать имя сервиса, т.к. на его основе будет создана DNS запись в кластере kubernetes
    - В селекторе мы описываем метки, по которым будут выбираться поды, чтобы перенаправлять на них трафик
- Чтобы наш трафик все-таки в какое-то приложение отправлялся, давайте запустим наше приложение:
    ```shell
    > kubectl create -f app
    
    deployment.apps/my-deployment created
    Error from server (AlreadyExists): error when creating "app/configmap.yaml": configmaps "my-configmap" already exists
    ```
    - Т.к. configmap уже существует, можно добиться нужного результата удалив созданные сущности, например через `kubectl delete -f app/` и затем повторив создание через команду `kubectl create -f app/`
    - Также можно выполнить `kubectl apply -f app/`
    - Если вдруг кто забыл, у нас есть две возможности создавать что-то в кластере kubernetes из файла
        - С помощью `kubectl create` и тогда объект в кластере создастся, если его ещё не было, а если этот объект уже был в кластере с этим именем, то он там не создастся
        - С помощью `kubectl apply` мы можем эти объекты конфигурировать, то есть если `kubectl apply -f` применить на какой-то объект, если этого объекта в кластере еще нет, то он создастся а если он уже есть, то сконфигурируется
- Посмотрим что у нас всё действительно запустилось:
    ```shell
    > kubectl get pod
    
    NAME                             READY   STATUS    RESTARTS   AGE
    my-deployment-5f979b576b-6shcg   1/1     Running   0          20m
    my-deployment-5f979b576b-mc4r6   1/1     Running   0          20m
    ```
- Чтобы лучше разобраться с ClusterIP, давайте воссоздадим соответствующие условия
- Мы развернули nginx, допустим что это приложение с каким-то фронтендом
- Запустим тестовое приложение, мы будем обращаться к основному приложению из этого пода с помощью установленных в нём утилит, таких как curl, nslookup
    ```shell
    > kubectl run test --image=centosadmin/utils:0.3
    
    # Если мы хотим сразу "провалиться" в командную строку в созданном поде, то это можно добавить в конце команды "-it -- bash"
    ```
- Мы можем узнать IP адрес пода с приложеним и постучаться по нему с помощью curl из пода с утилитами и получить ответ, но это будет работать до тех пор, пока под с приложением работает, как только под переедет или будет перезапущен, адрес изменится и такая возможность пропадёт
- Создаём сервис из манифеста, рассмотренного выше и посмотрим к чему это приведет
    ```shell
    > kubectl apply -f clusterip.yaml
    
    service/my-service created
    
    > kubectl get svc
    
    NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
    my-service   ClusterIP   10.254.115.220   <none>        80/TCP    2m48s
    ```
- При создании сервиса происходит следующее:
    - Ему присваивается некий IP адрес из заранее определенного пула (у нас кластере kubernetes есть как минимум два пула адресов, для подов и для сервисов и они не должны пересекаться)
    - В DNS кластера kubernetes создаётся DNS запись которая по имени сервиса будет отправлять нас в поды нашего приложения
- Итак, сервис создан, ему были присвоены IP адрес и DNS запись
- Не забываем про то, что сервис работает с подами используя селектор и метки на подах, они должны совпадать, смотрим что это так:
    ```shell
    > kubectl describe svc my-service | grep Selector

    Selector:          app=my-app

    > kubectl get pod --show-labels
    
    NAME                             READY   STATUS    RESTARTS   AGE   LABELS
    my-deployment-5f979b576b-6shcg   1/1     Running   0          66m   app=my-app,pod-template-hash=5f979b576b
    my-deployment-5f979b576b-mc4r6   1/1     Running   0          66m   app=my-app,pod-template-hash=5f979b576b
    test                             1/1     Running   1          26m   run=test
    ```
- Таким образом, сервис в рамках неймспейса будет перенаправлять запросы к нему на поды с соответствующей меткой
- Теперь мы можем снова зайти в наше под с утилитами и попробовать повзаимодействовать через сервис с nginx который запущен в нашем деплойменте 
    - Мы можем взять и использовать IP адрес сервиса, который мы видели выше в столбце CLUSTER-IP
        ```shell
        > kubectl exec -it test -- bash
        # тут мы провалились в под с утилитами и будем дёргать адрес балансироващика сервиса

        > curl 10.254.115.220

        my-deployment-5f979b576b-6shcg

        > curl 10.254.115.220

        my-deployment-5f979b576b-mc4r6
        ```
- Видим что всё работает,  у нас две реплики нашего приложения, отдается то один хостнейм, то другой, работает это рандомно, как подбрасывание монетки
- Но адрес сервиса тоже может смениться, хотя шанс этого и меньше
- Вспоминаем, что вместе с сервисом создаётся DNS запись, которая соответстует названию сервиса и не будет меняться, пока не изменится сам сервис, а если IP адрес сервиса изменится, то DNS запись обновится, это то что нам нужно, проверяем
    ```shell
    # мы всё ещй в поде с утилитами и теперь будем дёргать уже имя сервиса

    > curl my-service

    my-deployment-5f979b576b-6shcg

    > curl my-service

    my-deployment-5f979b576b-mc4r6
    ```
- Вспоминаем 12 factor app, конфигурирование приложений через переменые окружения, мы просто прописываем имена сервисов в переменных окружения, куда-то в db_host, frontend_host, во что нибудь такое и больше не заморачиваемся, вообще никаких айпи адресов, сервис-discovery у нас целиком лежит на kubernetes
- Удаляем за собой под с утилитами, он нам больше не нужен
    ```shell
    > kubectl delete pod test

    pod "test" deleted
    ```
- В рамках одного неймспейса мы можем обращаться к приложению через сервис указывая просто имя сервиса
- Eсли нужно обратиться к приложению в другом неймспейсе, то после имени сервиса через точку указываем имя неймспейса, вот так: `my-service.my-namespace`

#### Разбираем сервис NodePort [01:09:48](https://youtu.be/OmTYdf_uDeQ?t=4188)
- Смотрим в манифест:
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: my-service-np
    spec:
      ports:
      - port: 80
        targetPort: 80
      selector:
        app: my-app
      type: NodePort
    ```
    - Отличительная особенность - указание типа, а именно NodePort
    - Также мы можем явно указать порт ([документация](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport))
    - Всё остальное как у ClusterIP
- Пробуем применить манифест, получаем ошибку:
    ```shell
    > kubectl apply -f nodeport.yaml

    # часть ошибки не видна за аватаром ведущего, в момент написания конспекта ошибки уже не было
    Error from server (Forbidden): error when creating "nodeport.yaml": services "my-service-np" ...
    : exceeded quota: resource-quota, requested: service:nodeports=1, used: services.nodepo...
    services.nodeports=0
    ```
    - Учебный кластер готовили неглупые люди, ранее об этом упоминалось, есть инструменты администрирования кластера, это возможность обезопасить и себя и разработчиков от ошибок человеческого фактора, если мы приняли решение что в кластере нельзя создавать какой-то тип объекта или  объектов определённого типа должно быть ограниченное количество, то с помощью таких абстракций как ресурс квоты и лимит ренджи мы можем это достаточно жестко регламентировать
    - Вот прекрасный пример, мы захотели сервисов с типом NodePort в количестве 1, но у нас лимит на сервисы с типомNodePort в размере 0
- Ограничения сняты, пробуем применить манифест ещё раз:
    ```shell
    > kubectl apply -f nodeport.yaml

    service/my-service-np created
    ```
- Смотрим на сервисы:
    ```shell
    > kubectl get svc

    NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
    my-service      ClusterIP   10.254.115.220   <none>        80/TCP         102m
    my-service-np   NodePort    10.254.240.139   <none>        80:32405/TCP   18m
    ```
- Видим наш сервис, ему выданы IP адрес и DNS имя, которые работают так же как и в случае с сервисом типа ClusterIP
- Мы видим у нашего сервиса порт с номером больше 30000 и что он транслируется в 80 порт нашего приложения
- Если мы обратимся на адрес любой ноды в кластере на этот порт, то мы будем перенаправлены на наше приложение
    ```shell
    > kubectl get nodes -o wide | (head -1 && tail -1)

    NAME                                      STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP       OS-IMAGE         KERNEL-VERSION                CONTAINER-RUNTIME
    kubernetes-cluster-8675-master-2          Ready    master   36d   v1.20.4   10.0.0.5      213.219.212.215   CentOS Linux 8   4.18.0-305.7.1.el8_4.x86_64   docker://19.3.15

    # а это уже с личного ПК
    > curl 213.219.212.215:32405

    my-deployment-5f979b576b-6shcg

    > curl 213.219.212.215:32405

    my-deployment-5f979b576b-mc4r6
    ```
    - Это еще один повод запретить бездумное использование сервиса типа NodePort, потому что вот таким образом мы на всех эндпоинтах, на всех внешних IP адресах нашего кластера взяли и выставили свое приложение наружу, в реальности там мог бы быть дашборд какой-то или внутренняя система аутентификации или еще что-то, что могло бы послужить для хакеров входной точкой в наш проект, поэтому вполне логично, что подобные вещи надо как-то регламентировать, но это уже ближе к    администрированию

#### Разбираем сервис LoadBalancer [01:15:21](https://youtu.be/OmTYdf_uDeQ?t=4521)
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service-lb
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: my-app
  type: LoadBalancer
```
- Манифест практически идентичен предыдущим, отличия в типе сервиса
- Аналогичная предыдущим сервисам механика создания кластерного IP адреса и DNS имени
- Если мы попробуем применить данный манифест, то увидим, подобную картину:
    ```shell
    > kubectl get svc

    NAME            TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE
    my-service      ClusterIP      10.254.115.220   <none>          80/TCP         102m
    my-service-lb   LoadBalancer   10.254.206.240   <pending>       80:32465/TCP   8s
    my-service-np   NodePort       10.254.240.139   <none>          80:32405/TCP   18m
    ```
- В кластере вне облака, без специального облачного контроллера, pending будет висеть вечно, потому что никто извне не увидит что у нас создан сервис LoadBalancer и не создаст для него балансировщик с каким-то внешним IP адресом, чтобы мы через него могли попадать в наше приложение
- Когда шла речь про NodePort, упоминалось, что данный сервис используется в служебных целях с сервисом LoadBalancer. В списке портов мы видим трансляцию порта из диапазона выдачи портов NodePort, фактически, сервис LoadBalancer автоматизирует реализацию концепции с внешним балансировщиком и сервисом NodePort, за счёт применения облачного контроллера, который создаёт внешний балансировщик и настраивает его нужным образом для перенаправления трафика на транслируемые порты

### Резюме по сервисам [01:18:28](https://www.youtube.com/watch?v=OmTYdf_uDeQ&t=4708s)
- ![slurmy](./assets/lesson_6/slurmy.png)
- В результате полученной информации может сложится понимание, что сервис это разновидность прокси
- Действительно, у сервиса есть какой-то IP адрес и DNS-запись, если мы обращаемся к сервису, например через curl или из браузера, то он каким-то образом распределяет трафик на наши поды
- Таким образом, может сложится впечатление, что когда мы создаём сервис, то в кластере создаётся какой-то объект, например процесс балансировщика (haproxy или что-то на go), который всё это балансирует, но ничего такого не происходит, сервисы работают по другому
- Фактически, сервис из себя представляет правила в iptables или ipvs, при установке кластера можно выбрать, какой из этих бэкендов использовать
- Далее будет демонстрация на примере правил iptables, принципы в ipvs такие же
- Когда мы применяем манифест сервиса, например ClusterIP, на самом деле, в этот момент на всех нодах кластера создаются несколько правил iptables
- Если мы выполним на одной из нод соответствующую команду, например `iptables -t nat -A | grep my-service`, то мы сможем увидеть список из нескольких правил, давайте попробуем разобраться в этом и поймём, что сервис это не прокси
    - ![iptables1](./assets/lesson_6/iptables1.png)
        - Тут мы видим, что трафик, приходящий на IP адрес сервиса (1.1.1.1), на порт 80, по протоколу tcp, должен отправляться в определённую цепочку iptables (KUBE-SVC-UT6...)
    - ![iptables2](./assets/lesson_6/iptables2.png)
        - А здесь, что пришедший трафик будет с вероятностью 50% отправляться в другую цепочку (KUBE-SEP-MMY...), а тот трафик, что не был распределён, если правило вероятностного распределения не сработало, пойдет в следующую цепочку (KUBE-SEP-J33...)
    - ![iptables3](./assets/lesson_6/iptables3.png)
        - Тут мы видим, что пришедший трафик будет отправляться на определённый адрес и порт. Аналогичное правило мы увидим для другой цепочки (KUBE-SEP-J33...)
        - Адреса, указанные в этих правилах ни что иное, как адреса подов приложения
    - Итого, при создании сервиса у нас образуется несколько правил iptables которые через `--mode random probability <с какой-то вероятностью>` отправляет трафик на IP адреса наших подов по тем портам и протоколам, которые в этом сервисе были указаны. Если подов больше двух, то вероятности в правилах будут указаны соответствующие
    - Таким образом, мы работаем с привычным iptables или ipvs, а не с чем-то принципиально новым
    - На практике, погружаться на этот уровень практически не требуется
- Подытожим, сервис - это:
    - ![services](./assets/lesson_6/services.png)
    - Полная запись DNS имени сервиса представляет собой конструкцию вида **<имя сервиса>.<имя неймспейса>.svc.<доменное имя кластера>**, но с помощью неких механизмов, которые прописаны в resolv.conf каждого контейнера, мы можем просто обращаться по имени сервиса и попадать в нужное нам приложение или, если оно в другом неймспейсе, то просто **<имя сервиса>.<имя неймспейса>**, а полную конструкцию можно не указывать

### Ingress [01:25:29](https://www.youtube.com/watch?v=OmTYdf_uDeQ&t=5129s)
- ![ingress](./assets/lesson_6/ingress.png)
- Если вспомнить всё, о чём говорилось ранее, то становится понятно, что есть определённые способы публикации приложения, в том числе  наружу, но это не очень удобно, т.к. есть различные оговорки - определённый диапазон для публикации, нужен внешний балансировщик либо это применимо только в облаках
- А если у нас простое приложение, например интернет-магазин, чисто технически, мы можем создать специальный под с nginx, с помощью configmap пробросить конфигурацию, настроить нужные права доступа, чтобы у этого контейнера с nginx была возможность пробрасывать трафик извне в нужные поды с приложением
- На самом деле, именно так и работает IngressController, о котором дальше пойдет речь
- Если нам нужно опубликовать веб приложение наружу, то нам нужно использовать такой тип объекта как Ingress, а также запустить в кластере такой тип приложения как IngressController
- Здесь важно не путать:
    - Ingress - это манифест, абстракция
    - IngressController - это приложение, обычно это nginx, haproxy, envoy, traefic или что-то подобное, можно использовать то что удобно, либо то что подходит для реализации определённых специфических задач. Обычно установка и настройка IngressController ложится на плечи администраторов, поэтому мы не будем в это углубляться. Основное, что нужно понимать, что это такое же приложение, работающее в кластере, хорошо, если в несколько реплик, в это него приходят запросы и оно перенаправляет их дальше, в публикуемые приложения, к которым мы хотим предоставить доступ, пример на слайде демонстрирует, что разные эндпоинты перенаправят нас на разные приложения
- Как уже было сказано, IngressController необходимо как-то конфигурировать, под капотом работает какой нибудь nginx, у него есть конфиг и каждый раз его менять руками не удобно и вообще, так никто не делает. Для этого есть специальный объект Ingress, где мы в привычном формате yaml можем конфигурировать IngressController и явно указывать, какие запросы куда отправлять, давайте посмотрим на манифест такого объекта
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
  annotations:
    ngingx.ingress.kubernetes.io/backend-protocol: "HTTPS"
spec:
  rules:
  - host: foo.domain.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: my-service
            port:
              number: 80
```
- В разделе rules подраздела spec описано, как именно будет ходить трафик
    - host - хост, который будет обслуживать данный Ingress
    - pathType - используемый тип пути, их есть три типа, 2 из них самые распространённые
        - prefix
        - exact
    - В нашем случае все запросы, приходящие на корень, нужно адресовать на поды, которые стоят за сервисом my-service
    - В манифесте мы указываем имя сервиса, но только для того, чтобы IngressController определил, какие поды стоят за этим сервисом и сразу напрямую отправлял трафик в эти поды, сам сервис в этом не участвует
    - В рамках одного Ingress можно задавать несколько хостов
    - В рамках одного хоста мы можем задавать несколько путей
    - Может быть Ingress без хоста, тогда безадресные запросы будут попадать в этот Ingress
        - Это может быть опасно, подобным образом настроенный Ingress может быть использован в качестве входной точки для атакующих, подробнее здесь => [Дыры и заборы: безопасность в Kubernetes](https://www.youtube.com/watch?v=koTqZS-ThZ8)
- Про аннотации - в манифесте есть соответствующий раздел с длинной строкой
    - В манифестах kubernetes всё довольно строго типизировано, есть определённые наборы ключей и значений, с которым умеет работать API kubernetes, при этом, приложения, которые мы используем в кластере могут требовать дополнительной гибкости в настройке и стандартных директив манифестов нам не всегда хватает
    - Для решения вышеописанной проблемы и используются аннотации, мы можем записывать туда необходимые сведения для конфигурации наших приложений, в нашем примере мы указываем, что трафик будет работать по HTTPS
    - Варианты используемых аннотаций для nginx IngressController можно найти в [его документации](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#annotations)
    - При этом, иногда необходимо сконфигурировать нечто, для чего нет готовой аннотации, при этом сам nginx позволяет такое конфигурировать, например какой-то хитрый составной rewrite. Для подобного существует специальные аннотации snippet, [server snippet](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#server-snippet), позволяет передать в IngressController кусочек конфига, а [configuration snippet](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#configuration-snippet) позволяет сделать то же самое на уровне локейшна nginx
- По поводу безопасности соединения - HTTPS, TLS и т.п., в наше время важно обеспечивать безопасность соединения с помощью сертификатов
    - Такие манипуляции можно делать вручную:
        - ![ingressSecretTls](./assets/lesson_6/ingressSecretTls.png)
        - Выполнив команду внизу слайда мы создадим секрет, манифест которого описан справа, в котором в закодированном виде будут содержаться ключ и сертификат
        - Далее нужно указать данный секрет в секции tls Ingress (левый манифест на слайде), IngressController умеет работать с такими директивами и сделает всю грязную работу
    - Если же что-то подобное нужно автоматизировать (опять же, это скорее задача администраторов), можно использовать cert-manager, он регулярно ходит в центр сертификации, например letsencrypt, выписывает их оттуда и подключает в IngresController'ы. Подробнее об этом можно почитать в [документации cert-manager](https://cert-manager.io/docs/)
        - ![IngressCertManager](./assets/lesson_6/IngressCertManager.png)
        - Можно это делать через спец. объект Certificate, который появляется в кластере после установки cert-manager (левая часть слайда)
        - Либо через аннотации Ingress (правая часть слайда)

#### Запускаем Ingress [01:41:56](https://youtu.be/OmTYdf_uDeQ?t=6116)
```shell
$ cd ~/school-dev-k8s

$ git pull
...
17 files changed, 741 insertions(+)
...

$ cd practice/6.network-abstractions/2.ingress-and-services

$ vi host-ingress.yaml

$ cat host-ingress.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress-nginx
spec:
  rules:
  - host: my.s024713.mcs.slurm.io
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: my-service
            port:
              number: 80

$ k apply -f host-ingress.yaml

ingress.networking.k8s.io/my-ingress-nginx created
```
- После этого мы можем увидеть, что наше приложение доступно извне по адресу, указанному в поле host
- Мог возникнуть вопрос, можно ли с помощью Ingress опубликовывать приложения, которые работают поверх TCP/UDP, но не HTTP(S) - нет, в Ingress нет соответствующих полей, он задуман только для публикации приложений, работающих через http/https, мы даже можем это увидеть в документации. Если нужно реализовать что-то подобное, то можно использовать NodePort или LoadBalancer, о них было выше
    - Но, чисто технически, то что работает под капотом у IngressController, позволяет нам балансировать не только HTTP(S) трафик, так что при желании, с помощью специальных конфигмапов и аннотаций мы можем это реализовать. Но так делать не рекомендуется, это плохая практика, не Jedi-way

### Ещё кое что [01:44:54](https://youtu.be/OmTYdf_uDeQ?t=6294)
- Если что-то осталось непонятным, можно и нужно почитать официальную документацию по данным темам:
    - [ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)
    - [service](https://kubernetes.io/docs/concepts/services-networking/service/)
    - При желании, можно найти переведённые на русский разделы, но стоит иметь в виду, что перевод может быть не самым удачным
- [Подробнее про PathType](https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types) можно почитать в подразделе ingress, если кратко, то prefix менее, а exact более строгий
- Рекомендация на почитать: [Запуск проекта в Kubernetes за 60 минут](https://habr.com/ru/company/vk/blog/565250/)

## Вопросы [01:48:54](https://youtu.be/OmTYdf_uDeQ?t=6534)